{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17990f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'proxyUser': 'assumed-role_AdminNik_khokharn-Isengard', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f178e3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa94ca6d801a46279dbb22e4ee8f0c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1628248144302_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-55-110.ap-south-1.compute.internal:20888/proxy/application_1628248144302_0003/\" class=\"emr-proxy-link\" emr-resource=\"j-2MJRHFKHHVSS1\n",
       "\" application-id=\"application_1628248144302_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-58-135.ap-south-1.compute.internal:8042/node/containerlogs/container_1628248144302_0003_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cae509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60254ced56c049328f653ff4d3ee0fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1628248144302_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-55-110.ap-south-1.compute.internal:20888/proxy/application_1628248144302_0005/\" class=\"emr-proxy-link\" emr-resource=\"j-2MJRHFKHHVSS1\n",
       "\" application-id=\"application_1628248144302_0005\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-58-135.ap-south-1.compute.internal:8042/node/containerlogs/container_1628248144302_0005_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tableName = \"hudi_cow_table_mobikwik\"\n",
    "tablePath = \"s3://khokharn-hudi/MobiKwik/\" + tableName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24e201a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3933c2b84baf46859b4d542a2ecfebff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hudiWriteConfig = {\n",
    "    'className' : 'org.apache.hudi',\n",
    "    'hoodie.table.name': tableName,\n",
    "    'hoodie.datasource.write.operation': 'upsert',\n",
    "    'hoodie.datasource.write.table.type': 'COPY_ON_WRITE',\n",
    "    'hoodie.datasource.write.precombine.field': 'date',\n",
    "    'hoodie.datasource.write.recordkey.field': 'name',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'name:SIMPLE,year:SIMPLE,month:SIMPLE,day:SIMPLE',\n",
    "    'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.CustomKeyGenerator',\n",
    "    'hoodie.deltastreamer.keygen.timebased.timestamp.type': 'MIXED',\n",
    "    'hoodie.deltastreamer.keygen.timebased.input.dateformat': 'yyyy-mm-dd',\n",
    "    'hoodie.deltastreamer.keygen.timebased.output.dateformat':'yyyy/MM/dd'\n",
    "}\n",
    "     \n",
    "hudiGlueConfig = {\n",
    "    'hoodie.datasource.hive_sync.enable': 'true',\n",
    "    'hoodie.datasource.hive_sync.database': 'default',\n",
    "    'hoodie.datasource.hive_sync.table': tableName,\n",
    "    'hoodie.datasource.write.hive_style_partitioning' : 'true',\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n",
    "    'hoodie.datasource.hive_sync.partition_fields': 'name,year,month,day'\n",
    "}\n",
    "\n",
    "#'hoodie.datasource.hive_sync.jdbcurl': 'jdbc:hive2://localhost:10000',\n",
    "#'hoodie.datasource.write.partitionpath.field': 'name:SIMPLE,dt:TIMESTAMP',    \n",
    "\n",
    "combinedConf = {\n",
    "    **hudiWriteConfig, \n",
    "    **hudiGlueConfig\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bd9fda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9581d9fb184230a648557ca73edc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- col_to_update_integer: string (nullable = true)\n",
      " |-- col_to_update_string: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "|name   |date      |col_to_update_integer|col_to_update_string|year|month|day|\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "|Person1|2021-07-22|1234                 |White               |2021|7    |22 |\n",
      "|Person2|2021-07-22|1234                 |White               |2021|7    |22 |\n",
      "|Person3|2021-07-22|1234                 |White               |2021|7    |22 |\n",
      "|Person4|2021-07-22|1234                 |White               |2021|7    |22 |\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "# use for first run\n",
    "simpleData = [\n",
    "    (\"Person1\",\"2021-07-22\",\"1234\",\"White\"),\n",
    "    (\"Person2\",\"2021-07-22\",\"1234\",\"White\"),\n",
    "    (\"Person3\",\"2021-07-22\",\"1234\",\"White\"),\n",
    "    (\"Person4\",\"2021-07-22\",\"1234\",\"White\")\n",
    "]\n",
    "\n",
    "columns = [\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "\n",
    "df1 = df.select(\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\", year(df[\"date\"]).alias('year'), month(df[\"date\"]).alias('month'), dayofmonth(df[\"date\"]).alias('day'))\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "\n",
    "df1.write.format(\"hudi\") \\\n",
    ".options(**combinedConf) \\\n",
    ".mode(\"append\") \\\n",
    ".save(tablePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bd544c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09abdc624b3a4b358cbb8773365a8071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                        |name   |date      |col_to_update_integer|col_to_update_string|year|month|day|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "|20210808155456     |20210808155456_2_3  |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-228-72348_20210808155456.parquet|Person2|2021-07-22|1234                 |White               |2021|7    |22 |\n",
      "|20210808155456     |20210808155456_1_3  |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-228-72347_20210808155456.parquet|Person1|2021-07-22|1234                 |White               |2021|7    |22 |\n",
      "|20210808155456     |20210808155456_0_4  |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-228-72346_20210808155456.parquet|Person3|2021-07-22|1234                 |White               |2021|7    |22 |\n",
      "|20210808155456     |20210808155456_3_4  |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-228-72349_20210808155456.parquet|Person4|2021-07-22|1234                 |White               |2021|7    |22 |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "## Check via PySpark\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51fee0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fefd39e35a4ad0813b749e4b8e7ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- col_to_update_integer: string (nullable = true)\n",
      " |-- col_to_update_string: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "|name   |date      |col_to_update_integer|col_to_update_string|year|month|day|\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "|Person1|2021-07-22|4567                 |Yellow              |2021|7    |22 |\n",
      "|Person2|2021-07-22|4567                 |Yellow              |2021|7    |22 |\n",
      "|Person3|2021-07-22|4567                 |Yellow              |2021|7    |22 |\n",
      "|Person4|2021-07-22|4567                 |Yellow              |2021|7    |22 |\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "# use for second run for in-place update\n",
    "simpleData = [\n",
    "    (\"Person1\",\"2021-07-22\",\"4567\",\"Yellow\"),\n",
    "    (\"Person2\",\"2021-07-22\",\"4567\",\"Yellow\"),\n",
    "    (\"Person3\",\"2021-07-22\",\"4567\",\"Yellow\"),\n",
    "    (\"Person4\",\"2021-07-22\",\"4567\",\"Yellow\")\n",
    "]\n",
    "\n",
    "columns = [\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "\n",
    "df1 = df.select(\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\", year(df[\"date\"]).alias('year'), month(df[\"date\"]).alias('month'), dayofmonth(df[\"date\"]).alias('day'))\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "\n",
    "df1.write.format(\"hudi\") \\\n",
    ".options(**combinedConf) \\\n",
    ".mode(\"append\") \\\n",
    ".save(tablePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef5bc77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781493eb90b54e66a7526fcc0c2af54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                        |name   |date      |col_to_update_integer|col_to_update_string|year|month|day|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "|20210808155922     |20210808155922_0_5  |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-265-85904_20210808155922.parquet|Person3|2021-07-22|4567                 |Yellow              |2021|7    |22 |\n",
      "|20210808155922     |20210808155922_2_6  |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-265-85906_20210808155922.parquet|Person2|2021-07-22|4567                 |Yellow              |2021|7    |22 |\n",
      "|20210808155922     |20210808155922_1_6  |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-265-85905_20210808155922.parquet|Person1|2021-07-22|4567                 |Yellow              |2021|7    |22 |\n",
      "|20210808155922     |20210808155922_3_5  |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-265-85907_20210808155922.parquet|Person4|2021-07-22|4567                 |Yellow              |2021|7    |22 |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "## Check via PySpark\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8ac4d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e2a1e6f20e4dfcaf1b603f14c69b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- col_to_update_integer: string (nullable = true)\n",
      " |-- col_to_update_string: string (nullable = true)\n",
      " |-- new_col: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n",
      "+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|name   |date      |col_to_update_integer|col_to_update_string|new_col|year|month|day|\n",
      "+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|Person1|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "|Person2|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "|Person3|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "|Person4|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "+-------+----------+---------------------+--------------------+-------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "# use for forward schema evolution\n",
    "# new columns are accept all good\n",
    "simpleData = [\n",
    "    (\"Person1\",\"2021-07-22\",\"8910\",\"Silver\",\"abc\"),\n",
    "    (\"Person2\",\"2021-07-22\",\"8910\",\"Silver\",\"abc\"),\n",
    "    (\"Person3\",\"2021-07-22\",\"8910\",\"Silver\",\"abc\"),\n",
    "    (\"Person4\",\"2021-07-22\",\"8910\",\"Silver\",\"abc\")\n",
    "]\n",
    "\n",
    "columns = [\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\",\"new_col\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "\n",
    "df1 = df.select(\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\",\"new_col\", year(df[\"date\"]).alias('year'), month(df[\"date\"]).alias('month'), dayofmonth(df[\"date\"]).alias('day'))\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "\n",
    "df1.write.format(\"hudi\") \\\n",
    ".options(**combinedConf) \\\n",
    ".mode(\"append\") \\\n",
    ".save(tablePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db8bf613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a584c027e94d45aa24be6db41b378a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                         |name   |date      |col_to_update_integer|col_to_update_string|new_col|year|month|day|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|20210808160422     |20210808160422_0_10 |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-336-113015_20210808160422.parquet|Person3|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "|20210808160422     |20210808160422_3_10 |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-336-113018_20210808160422.parquet|Person4|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "|20210808160422     |20210808160422_2_9  |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-336-113017_20210808160422.parquet|Person2|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "|20210808160422     |20210808160422_1_9  |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-336-113016_20210808160422.parquet|Person1|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "## Check via PySpark\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03db293d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfe64301f884f9f8d717d8f049b06c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o843.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 373.0 failed 4 times, most recent failure: Lost task 0.3 in stage 373.0 (TID 126585) (ip-172-31-58-135.ap-south-1.compute.internal executor 10): org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:279)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$execute$ecf5068c$1(BaseSparkCommitActionExecutor.java:135)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1440)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.action.commit.SparkMergeHelper.runMerge(SparkMergeHelper.java:102)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdateInternal(BaseSparkCommitActionExecutor.java:308)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdate(BaseSparkCommitActionExecutor.java:299)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:272)\n",
      "\t... 28 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:143)\n",
      "\tat org.apache.hudi.table.action.commit.SparkMergeHelper.runMerge(SparkMergeHelper.java:100)\n",
      "\t... 31 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:141)\n",
      "\t... 32 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:247)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:226)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:52)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:277)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:36)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:121)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'new_col' not found\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:225)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:130)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)\n",
      "\tat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\n",
      "\tat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\n",
      "\tat org.apache.hudi.common.util.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:49)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:45)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:92)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.commitAndPerformPostOperations(HoodieSparkSqlWriter.scala:433)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:218)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:134)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:279)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$execute$ecf5068c$1(BaseSparkCommitActionExecutor.java:135)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1440)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.action.commit.SparkMergeHelper.runMerge(SparkMergeHelper.java:102)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdateInternal(BaseSparkCommitActionExecutor.java:308)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdate(BaseSparkCommitActionExecutor.java:299)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:272)\n",
      "\t... 28 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:143)\n",
      "\tat org.apache.hudi.table.action.commit.SparkMergeHelper.runMerge(SparkMergeHelper.java:100)\n",
      "\t... 31 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:141)\n",
      "\t... 32 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:247)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:226)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:52)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:277)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:36)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:121)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'new_col' not found\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:225)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:130)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)\n",
      "\tat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\n",
      "\tat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\n",
      "\tat org.apache.hudi.common.util.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:49)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:45)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:92)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1109, in save\n",
      "    self._jwrite.save(path)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o843.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 373.0 failed 4 times, most recent failure: Lost task 0.3 in stage 373.0 (TID 126585) (ip-172-31-58-135.ap-south-1.compute.internal executor 10): org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:279)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$execute$ecf5068c$1(BaseSparkCommitActionExecutor.java:135)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1440)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.action.commit.SparkMergeHelper.runMerge(SparkMergeHelper.java:102)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdateInternal(BaseSparkCommitActionExecutor.java:308)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdate(BaseSparkCommitActionExecutor.java:299)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:272)\n",
      "\t... 28 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:143)\n",
      "\tat org.apache.hudi.table.action.commit.SparkMergeHelper.runMerge(SparkMergeHelper.java:100)\n",
      "\t... 31 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:141)\n",
      "\t... 32 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:247)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:226)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:52)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:277)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:36)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:121)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'new_col' not found\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:225)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:130)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)\n",
      "\tat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\n",
      "\tat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\n",
      "\tat org.apache.hudi.common.util.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:49)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:45)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:92)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.commitAndPerformPostOperations(HoodieSparkSqlWriter.scala:433)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:218)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:134)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:279)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$execute$ecf5068c$1(BaseSparkCommitActionExecutor.java:135)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1440)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.table.action.commit.SparkMergeHelper.runMerge(SparkMergeHelper.java:102)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdateInternal(BaseSparkCommitActionExecutor.java:308)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdate(BaseSparkCommitActionExecutor.java:299)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:272)\n",
      "\t... 28 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:143)\n",
      "\tat org.apache.hudi.table.action.commit.SparkMergeHelper.runMerge(SparkMergeHelper.java:100)\n",
      "\t... 31 more\n",
      "Caused by: java.util.concurrent.ExecutionException: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.execute(BoundedInMemoryExecutor.java:141)\n",
      "\t... 32 more\n",
      "Caused by: org.apache.hudi.exception.HoodieException: operation has failed\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.throwExceptionIfFailed(BoundedInMemoryQueue.java:247)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.readNextRecord(BoundedInMemoryQueue.java:226)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue.access$100(BoundedInMemoryQueue.java:52)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueue$QueueIterator.hasNext(BoundedInMemoryQueue.java:277)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryQueueConsumer.consume(BoundedInMemoryQueueConsumer.java:36)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$2(BoundedInMemoryExecutor.java:121)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'new_col' not found\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.getAvroField(AvroRecordConverter.java:225)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:130)\n",
      "\tat org.apache.parquet.avro.AvroRecordConverter.<init>(AvroRecordConverter.java:95)\n",
      "\tat org.apache.parquet.avro.AvroRecordMaterializer.<init>(AvroRecordMaterializer.java:33)\n",
      "\tat org.apache.parquet.avro.AvroReadSupport.prepareForRead(AvroReadSupport.java:138)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:183)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.initReader(ParquetReader.java:156)\n",
      "\tat org.apache.parquet.hadoop.ParquetReader.read(ParquetReader.java:135)\n",
      "\tat org.apache.hudi.common.util.ParquetReaderIterator.hasNext(ParquetReaderIterator.java:49)\n",
      "\tat org.apache.hudi.common.util.queue.IteratorBasedQueueProducer.produce(IteratorBasedQueueProducer.java:45)\n",
      "\tat org.apache.hudi.common.util.queue.BoundedInMemoryExecutor.lambda$null$0(BoundedInMemoryExecutor.java:92)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\t... 4 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use for backward schema evolution \n",
    "# fails if newly added column is no longer found \n",
    "# Caused by: org.apache.parquet.io.InvalidRecordException: Parquet/Avro schema mismatch: Avro field 'new_col' not found\n",
    "simpleData = [\n",
    "    (\"Person1\",\"2021-07-22\",\"11121314\",\"Purple\"),\n",
    "    (\"Person2\",\"2021-07-22\",\"11121314\",\"Purple\"),\n",
    "    (\"Person3\",\"2021-07-22\",\"11121314\",\"Purple\"),\n",
    "    (\"Person4\",\"2021-07-22\",\"11121314\",\"Purple\")\n",
    "]\n",
    "\n",
    "columns = [\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "\n",
    "df1 = df.select(\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\", year(df[\"date\"]).alias('year'), month(df[\"date\"]).alias('month'), dayofmonth(df[\"date\"]).alias('day'))\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "\n",
    "df1.write.format(\"hudi\") \\\n",
    ".options(**combinedConf) \\\n",
    ".mode(\"append\") \\\n",
    ".save(tablePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9de9cd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665153df79f342b689dd30d44e24b553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ensure the incomong record has the correct current schema, new fresh columns are fine, if a column exists in current schema but not in incoming record then manually add before inserting\n",
    "def evolveSchema(df,table,forcecast=False):\n",
    "    try:\n",
    "        #get existing table's schema\n",
    "        print(\"\\nexisting schema in hive is :\")\n",
    "        original_df = spark.sql(\"SELECT * FROM \"+table+\" LIMIT 0\")\n",
    "        original_df.printSchema()\n",
    "        \n",
    "        #sanitize for hudi specific system columns\n",
    "        print(\"\\nexisting schema in hive (sanitized for hudi columns) is :\")\n",
    "        columns_to_drop = ['_hoodie_commit_time', '_hoodie_commit_seqno','_hoodie_record_key','_hoodie_partition_path','_hoodie_file_name']\n",
    "        odf = original_df.drop(*columns_to_drop)\n",
    "        odf.printSchema()\n",
    "        \n",
    "        if (df.schema != odf.schema):\n",
    "            merged_df = df.unionByName(odf, allowMissingColumns=True)\n",
    "\n",
    "        return (merged_df)\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        return (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a44fa7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2244d1ba658a468a8e41fbfa2889bd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- col_to_update_integer: string (nullable = true)\n",
      " |-- col_to_update_string: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "|name   |date      |col_to_update_integer|col_to_update_string|year|month|day|\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "|Person1|2021-07-22|11121314             |Purple              |2021|7    |22 |\n",
      "|Person2|2021-07-22|11121314             |Purple              |2021|7    |22 |\n",
      "|Person3|2021-07-22|11121314             |Purple              |2021|7    |22 |\n",
      "|Person4|2021-07-22|11121314             |Purple              |2021|7    |22 |\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+\n",
      "\n",
      "\n",
      "existing schema in hive is :\n",
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- col_to_update_integer: string (nullable = true)\n",
      " |-- col_to_update_string: string (nullable = true)\n",
      " |-- new_col: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n",
      "\n",
      "existing schema in hive (sanitized for hudi columns) is :\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- col_to_update_integer: string (nullable = true)\n",
      " |-- col_to_update_string: string (nullable = true)\n",
      " |-- new_col: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- col_to_update_integer: string (nullable = true)\n",
      " |-- col_to_update_string: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- new_col: string (nullable = true)\n",
      "\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+-------+\n",
      "|name   |date      |col_to_update_integer|col_to_update_string|year|month|day|new_col|\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+-------+\n",
      "|Person1|2021-07-22|11121314             |Purple              |2021|7    |22 |null   |\n",
      "|Person2|2021-07-22|11121314             |Purple              |2021|7    |22 |null   |\n",
      "|Person3|2021-07-22|11121314             |Purple              |2021|7    |22 |null   |\n",
      "|Person4|2021-07-22|11121314             |Purple              |2021|7    |22 |null   |\n",
      "+-------+----------+---------------------+--------------------+----+-----+---+-------+"
     ]
    }
   ],
   "source": [
    "# use for backward schema evolution \n",
    "# manually add default values for columns which exist in schema but not in new records\n",
    "\n",
    "simpleData = [\n",
    "    (\"Person1\",\"2021-07-22\",\"11121314\",\"Purple\"),\n",
    "    (\"Person2\",\"2021-07-22\",\"11121314\",\"Purple\"),\n",
    "    (\"Person3\",\"2021-07-22\",\"11121314\",\"Purple\"),\n",
    "    (\"Person4\",\"2021-07-22\",\"11121314\",\"Purple\")\n",
    "]\n",
    "\n",
    "columns = [\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "\n",
    "df1 = df.select(\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\", year(df[\"date\"]).alias('year'), month(df[\"date\"]).alias('month'), dayofmonth(df[\"date\"]).alias('day'))\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "df2 = evolveSchema(df1,tableName,False)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df2.write.format(\"hudi\") \\\n",
    ".options(**combinedConf) \\\n",
    ".mode(\"append\") \\\n",
    ".save(tablePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c4bf5d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3c0c9e57ae4fae92a562c1de63dc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+----+-----+---+-------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                         |name   |date      |col_to_update_integer|col_to_update_string|year|month|day|new_col|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+----+-----+---+-------+\n",
      "|20210808171322     |20210808171322_3_2  |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-510-167394_20210808171322.parquet|Person4|2021-07-22|11121314             |Purple              |2021|7    |22 |null   |\n",
      "|20210808171322     |20210808171322_0_2  |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-510-167391_20210808171322.parquet|Person3|2021-07-22|11121314             |Purple              |2021|7    |22 |null   |\n",
      "|20210808171322     |20210808171322_2_1  |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-510-167393_20210808171322.parquet|Person2|2021-07-22|11121314             |Purple              |2021|7    |22 |null   |\n",
      "|20210808171322     |20210808171322_1_1  |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-510-167392_20210808171322.parquet|Person1|2021-07-22|11121314             |Purple              |2021|7    |22 |null   |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+----+-----+---+-------+"
     ]
    }
   ],
   "source": [
    "## Check via PySpark\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ad5f1e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc36cb4c035461183a2b26abfc1a152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- col_to_update_integer: string (nullable = true)\n",
      " |-- col_to_update_string: string (nullable = true)\n",
      " |-- new_col: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n",
      "+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|name   |date      |col_to_update_integer|col_to_update_string|new_col|year|month|day|\n",
      "+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|Person1|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "|Person2|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "|Person3|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "|Person4|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "+-------+----------+---------------------+--------------------+-------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "# use for backward schema evolution \n",
    "# manually add default values for columns which exist in schema but not in new records\n",
    "\n",
    "simpleData = [\n",
    "    (\"Person1\",\"2021-07-22\",\"15161718\",\"Orange\",\"again\"),\n",
    "    (\"Person2\",\"2021-07-22\",\"15161718\",\"Orange\",\"again\"),\n",
    "    (\"Person3\",\"2021-07-22\",\"15161718\",\"Orange\",\"again\"),\n",
    "    (\"Person4\",\"2021-07-22\",\"15161718\",\"Orange\",\"again\")\n",
    "]\n",
    "\n",
    "columns = [\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\",\"new_col\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "\n",
    "df1 = df.select(\"name\",\"date\",\"col_to_update_integer\",\"col_to_update_string\", \"new_col\", year(df[\"date\"]).alias('year'), month(df[\"date\"]).alias('month'), dayofmonth(df[\"date\"]).alias('day'))\n",
    "df1.printSchema()\n",
    "df1.show(truncate=False)\n",
    "\n",
    "#df2 = evolveSchema(df1,tableName,False)\n",
    "#df2.printSchema()\n",
    "#df2.show(truncate=False)\n",
    "\n",
    "df1.write.format(\"hudi\") \\\n",
    ".options(**combinedConf) \\\n",
    ".mode(\"append\") \\\n",
    ".save(tablePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b2180dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a56ad3ae4ae4a5b85162e0241a7ea37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                         |name   |date      |col_to_update_integer|col_to_update_string|new_col|year|month|day|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|20210808171924     |20210808171924_0_12 |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-581-194531_20210808171924.parquet|Person3|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "|20210808171924     |20210808171924_2_11 |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-581-194533_20210808171924.parquet|Person2|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "|20210808171924     |20210808171924_1_12 |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-581-194532_20210808171924.parquet|Person1|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "|20210808171924     |20210808171924_3_11 |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-581-194534_20210808171924.parquet|Person4|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "## Check via PySpark\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b962a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47819a31f8484fe796258e9953c6622b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                        |name   |date      |col_to_update_integer|col_to_update_string|new_col|year|month|day|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|20210808155456     |20210808155456_2_3  |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-228-72348_20210808155456.parquet|Person2|2021-07-22|1234                 |White               |null   |2021|7    |22 |\n",
      "|20210808155456     |20210808155456_1_3  |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-228-72347_20210808155456.parquet|Person1|2021-07-22|1234                 |White               |null   |2021|7    |22 |\n",
      "|20210808155456     |20210808155456_0_4  |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-228-72346_20210808155456.parquet|Person3|2021-07-22|1234                 |White               |null   |2021|7    |22 |\n",
      "|20210808155456     |20210808155456_3_4  |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-228-72349_20210808155456.parquet|Person4|2021-07-22|1234                 |White               |null   |2021|7    |22 |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "#TimeTravel\n",
    "# available commit timestamps in .hoodie folder of S3 :\n",
    "# 20210808155456\t\n",
    "# 20210808155922\n",
    "# 20210808160202\n",
    "# 20210808160422\n",
    "# 20210808171322\n",
    "# 20210808171924\n",
    "\n",
    "starttime = \"0000\"\n",
    "endtime = \"20210808155456\"\n",
    "\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    ".option(\"hoodie.datasource.read.begin.instanttime\", starttime) \\\n",
    ".option(\"hoodie.datasource.read.end.instanttime\", endtime) \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6248af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0942c94408fd4d31af72d44e5cd3a931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                        |name   |date      |col_to_update_integer|col_to_update_string|new_col|year|month|day|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|20210808155922     |20210808155922_2_6  |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-265-85906_20210808155922.parquet|Person2|2021-07-22|4567                 |Yellow              |null   |2021|7    |22 |\n",
      "|20210808155922     |20210808155922_0_5  |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-265-85904_20210808155922.parquet|Person3|2021-07-22|4567                 |Yellow              |null   |2021|7    |22 |\n",
      "|20210808155922     |20210808155922_1_6  |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-265-85905_20210808155922.parquet|Person1|2021-07-22|4567                 |Yellow              |null   |2021|7    |22 |\n",
      "|20210808155922     |20210808155922_3_5  |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-265-85907_20210808155922.parquet|Person4|2021-07-22|4567                 |Yellow              |null   |2021|7    |22 |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "starttime = \"20210808155456\"\n",
    "endtime = \"20210808155922\"\n",
    "\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    ".option(\"hoodie.datasource.read.begin.instanttime\", starttime) \\\n",
    ".option(\"hoodie.datasource.read.end.instanttime\", endtime) \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39654696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02bd2627cd94837aa9cf7cc59c5a0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                        |name   |date      |col_to_update_integer|col_to_update_string|new_col|year|month|day|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|20210808160202     |20210808160202_1_8  |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-302-99463_20210808160202.parquet|Person1|2021-07-22|8910                 |Silver              |null   |2021|7    |22 |\n",
      "|20210808160202     |20210808160202_0_8  |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-302-99462_20210808160202.parquet|Person3|2021-07-22|8910                 |Silver              |null   |2021|7    |22 |\n",
      "|20210808160202     |20210808160202_2_7  |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-302-99464_20210808160202.parquet|Person2|2021-07-22|8910                 |Silver              |null   |2021|7    |22 |\n",
      "|20210808160202     |20210808160202_3_7  |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-302-99465_20210808160202.parquet|Person4|2021-07-22|8910                 |Silver              |null   |2021|7    |22 |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+-------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "starttime = \"20210808155922\"\n",
    "endtime = \"20210808160202\"\n",
    "\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    ".option(\"hoodie.datasource.read.begin.instanttime\", starttime) \\\n",
    ".option(\"hoodie.datasource.read.end.instanttime\", endtime) \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b75bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba12d7ce3bb497fab39093d65e694b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                         |name   |date      |col_to_update_integer|col_to_update_string|new_col|year|month|day|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|20210808160422     |20210808160422_0_10 |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-336-113015_20210808160422.parquet|Person3|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "|20210808160422     |20210808160422_3_10 |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-336-113018_20210808160422.parquet|Person4|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "|20210808160422     |20210808160422_2_9  |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-336-113017_20210808160422.parquet|Person2|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "|20210808160422     |20210808160422_1_9  |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-336-113016_20210808160422.parquet|Person1|2021-07-22|8910                 |Silver              |abc    |2021|7    |22 |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "starttime = \"20210808160202\"\n",
    "endtime = \"20210808160422\"\n",
    "\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    ".option(\"hoodie.datasource.read.begin.instanttime\", starttime) \\\n",
    ".option(\"hoodie.datasource.read.end.instanttime\", endtime) \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5eacf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd806a0d564f470a98cd3801b6214694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                         |name   |date      |col_to_update_integer|col_to_update_string|new_col|year|month|day|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|20210808171322     |20210808171322_2_1  |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-510-167393_20210808171322.parquet|Person2|2021-07-22|11121314             |Purple              |null   |2021|7    |22 |\n",
      "|20210808171322     |20210808171322_0_2  |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-510-167391_20210808171322.parquet|Person3|2021-07-22|11121314             |Purple              |null   |2021|7    |22 |\n",
      "|20210808171322     |20210808171322_3_2  |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-510-167394_20210808171322.parquet|Person4|2021-07-22|11121314             |Purple              |null   |2021|7    |22 |\n",
      "|20210808171322     |20210808171322_1_1  |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-510-167392_20210808171322.parquet|Person1|2021-07-22|11121314             |Purple              |null   |2021|7    |22 |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "starttime = \"20210808160422\"\n",
    "endtime = \"20210808171322\"\n",
    "\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    ".option(\"hoodie.datasource.read.begin.instanttime\", starttime) \\\n",
    ".option(\"hoodie.datasource.read.end.instanttime\", endtime) \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06065b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8739e2f9147c4a8ca4878453a10e3129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path               |_hoodie_file_name                                                         |name   |date      |col_to_update_integer|col_to_update_string|new_col|year|month|day|\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+\n",
      "|20210808171924     |20210808171924_2_11 |Person2           |name=Person2/year=2021/month=7/day=22|c2f7bac6-24ab-4661-8791-4e299781dd92-0_2-581-194533_20210808171924.parquet|Person2|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "|20210808171924     |20210808171924_0_12 |Person3           |name=Person3/year=2021/month=7/day=22|f265b2d9-0b54-4c0d-bdf8-25326d6454e9-0_0-581-194531_20210808171924.parquet|Person3|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "|20210808171924     |20210808171924_1_12 |Person1           |name=Person1/year=2021/month=7/day=22|7e28e1f2-9cee-4b6c-ada0-3bf587f8aa7c-0_1-581-194532_20210808171924.parquet|Person1|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "|20210808171924     |20210808171924_3_11 |Person4           |name=Person4/year=2021/month=7/day=22|a1544097-1394-4214-8de5-43aa8f27f273-0_3-581-194534_20210808171924.parquet|Person4|2021-07-22|15161718             |Orange              |again  |2021|7    |22 |\n",
      "+-------------------+--------------------+------------------+-------------------------------------+--------------------------------------------------------------------------+-------+----------+---------------------+--------------------+-------+----+-----+---+"
     ]
    }
   ],
   "source": [
    "starttime = \"20210808171322\"\n",
    "endtime = \"20210808171924\"\n",
    "\n",
    "hudiDF = spark.read \\\n",
    ".format(\"hudi\") \\\n",
    ".option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    ".option(\"hoodie.datasource.read.begin.instanttime\", starttime) \\\n",
    ".option(\"hoodie.datasource.read.end.instanttime\", endtime) \\\n",
    ".load(tablePath).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d55dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
